{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Party_Extractor_separate.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PUC4Lkm_CPe8"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUC4Lkm_CPe8"
      },
      "source": [
        "## Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKiy8_Uj7Mjm",
        "outputId": "b39a57b1-adce-4ccd-92cc-cbb5fc5cc63e"
      },
      "source": [
        "!wget -P /content/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-06 04:27:15--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.128.208\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.128.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/content/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  45.8MB/s    in 35s     \n",
            "\n",
            "2021-05-06 04:27:51 (44.8 MB/s) - ‘/content/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCNzggJW_KYD",
        "outputId": "c57dfd73-9c7a-4276-84de-694da71f999a"
      },
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip https://nlp.stanford.edu/software/stanford-english-corenlp-2018-10-05-models.jar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-06 04:43:20--  https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2018-10-05.zip [following]\n",
            "--2021-05-06 04:43:21--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 393239982 (375M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-full-2018-10-05.zip’\n",
            "\n",
            "stanford-corenlp-fu 100%[===================>] 375.02M  4.84MB/s    in 73s     \n",
            "\n",
            "2021-05-06 04:44:33 (5.16 MB/s) - ‘stanford-corenlp-full-2018-10-05.zip’ saved [393239982/393239982]\n",
            "\n",
            "--2021-05-06 04:44:33--  https://nlp.stanford.edu/software/stanford-english-corenlp-2018-10-05-models.jar\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-english-corenlp-2018-10-05-models.jar [following]\n",
            "--2021-05-06 04:44:33--  https://downloads.cs.stanford.edu/nlp/software/stanford-english-corenlp-2018-10-05-models.jar\n",
            "Reusing existing connection to downloads.cs.stanford.edu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1038970602 (991M) [application/java-archive]\n",
            "Saving to: ‘stanford-english-corenlp-2018-10-05-models.jar’\n",
            "\n",
            "stanford-english-co 100%[===================>] 990.84M  5.04MB/s    in 3m 18s  \n",
            "\n",
            "2021-05-06 04:47:52 (5.00 MB/s) - ‘stanford-english-corenlp-2018-10-05-models.jar’ saved [1038970602/1038970602]\n",
            "\n",
            "FINISHED --2021-05-06 04:47:52--\n",
            "Total wall clock time: 4m 31s\n",
            "Downloaded: 2 files, 1.3G in 4m 31s (5.04 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pyld-7-AW9J",
        "outputId": "6b76e845-f616-43d1-f09c-421bf12aa6d4"
      },
      "source": [
        "!unzip stanford-corenlp-full-2018-10-05.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  stanford-corenlp-full-2018-10-05.zip\n",
            "   creating: stanford-corenlp-full-2018-10-05/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/README.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2018-10-05/sutime/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.holidays.sutime.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/build.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-javadoc.jar  \n",
            "   creating: stanford-corenlp-full-2018-10-05/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2018-10-05/patterns/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/Makefile  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LICENSE.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxUG8skoAjvM"
      },
      "source": [
        "!mv stanford-english-corenlp-2018-10-05-models.jar stanford-corenlp-full-2018-10-05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnWqKpt_HGNj",
        "outputId": "a8ac3113-3db0-43ca-faf0-9ff87c0df535"
      },
      "source": [
        "!wget https://osf.io/yemvd/download"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-06 05:18:18--  https://osf.io/yemvd/download\n",
            "Resolving osf.io (osf.io)... 35.190.84.173\n",
            "Connecting to osf.io (osf.io)|35.190.84.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://files.osf.io/v1/resources/mvw7a/providers/osfstorage/6091468d19183d03395523c4?action=download&direct&version=1 [following]\n",
            "--2021-05-06 05:18:18--  https://files.osf.io/v1/resources/mvw7a/providers/osfstorage/6091468d19183d03395523c4?action=download&direct&version=1\n",
            "Resolving files.osf.io (files.osf.io)... 35.186.214.196\n",
            "Connecting to files.osf.io (files.osf.io)|35.186.214.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28223110 (27M) [application/octet-stream]\n",
            "Saving to: ‘download’\n",
            "\n",
            "download            100%[===================>]  26.92M  33.7MB/s    in 0.8s    \n",
            "\n",
            "2021-05-06 05:18:20 (33.7 MB/s) - ‘download’ saved [28223110/28223110]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApeRQj8sHQuJ",
        "outputId": "4c151fa3-e77d-4eb5-8c25-7c3e21f771fc"
      },
      "source": [
        "!unzip download"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  download\n",
            "   creating: GRU_512/assets/\n",
            "  inflating: GRU_512/variables/variables.index  \n",
            "  inflating: GRU_512/saved_model.pb  \n",
            "  inflating: GRU_512/variables/variables.data-00000-of-00001  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HU3lFWXCI4j"
      },
      "source": [
        "## Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ4nfXBxXwd7",
        "outputId": "c1bea808-6b9c-4c8e-8ddf-f86ffb7345d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mLLr94ifvYK"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7EZmmJTSOkj"
      },
      "source": [
        "from random import randint\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, Dense, Maximum, LayerNormalization\n",
        "from keras.utils import *\n",
        "from keras.initializers import *\n",
        "import tensorflow as tf\n",
        "import time, random\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_aPnjR5MgJv"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# data_path = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"Data_W2V\"\n",
        "# Data_path = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"classifier\" / \"data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN7u861oGGJB"
      },
      "source": [
        "GOOGLE_NEWS_VECTORS_PATH = \"/content/drive/MyDrive/data/GoogleNews-vectors-negative300.bin.gz\"\n",
        "STANFORD_CORENLP = \"/content/drive/MyDrive/data/stanford-corenlp-full-2018-10-05/\"\n",
        "GRU_512_MODEL = \"/content/drive/MyDrive/data/GRU_512/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPtze42TYFOA",
        "outputId": "6bfa8806-6917-4baa-f7e7-f849ca53516c"
      },
      "source": [
        "!pip install stanfordcorenlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanfordcorenlp\n",
            "  Downloading https://files.pythonhosted.org/packages/35/cb/0a271890bbe3a77fc1aca2bc3a58b14e11799ea77cb5f7d6fb0a8b4c46fa/stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stanfordcorenlp) (5.4.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordcorenlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (3.0.4)\n",
            "Installing collected packages: stanfordcorenlp\n",
            "Successfully installed stanfordcorenlp-3.9.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KxFFUWCf-kP"
      },
      "source": [
        "### StanfordCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCsq3NxOcqv-"
      },
      "source": [
        "import json\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "\n",
        "# nlp = StanfordCoreNLP(r'/content/drive/Shared drives/SigmaLaw/stanford-corenlp-full-2018-10-05', quiet=False)\n",
        "nlp = StanfordCoreNLP(STANFORD_CORENLP, quiet=False)\n",
        "props = {'annotators': 'tokenize, ner, coref', 'pipelineLanguage': 'en'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe6Ooq1cgEpS"
      },
      "source": [
        "### Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5YNaubPxvUD"
      },
      "source": [
        "# DATA_PATH = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "import gensim\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format(DATA_PATH / 'GoogleNews-vectors-negative300.bin', binary=True, unicode_errors='ignore')\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_NEWS_VECTORS_PATH, binary=True, unicode_errors='ignore')\n",
        "from gensim.models import Word2Vec "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6hE8nBDgMYe"
      },
      "source": [
        "### Main Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j110IaUdFUA"
      },
      "source": [
        "def build_complete_ner(word, ner, l):\n",
        "  if (len(l)>0):\n",
        "    if (l[0].get('ner') == ner):\n",
        "      word = word + \" \" + build_complete_ner(l[0].get(\"word\"), l[0].get('ner'), l[1:len(l)])\n",
        "    #print(\"built\" + word)\n",
        "    return word\n",
        "  else:\n",
        "    return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeFizBmPdPyC"
      },
      "source": [
        "def remove_non_ascii_1(text):\n",
        "  return ''.join(i for i in text if ord(i)<128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpjzPdgC0RHn"
      },
      "source": [
        "def getMaskValues(ner_dict):\n",
        "  masks_dict = {}\n",
        "  keys = list(ner_dict.keys())\n",
        "  for i in range(len(keys)):\n",
        "    if ner_dict[keys[i]] == \"P\":\n",
        "      value = 0.5*((1+i)/len(keys))\n",
        "    elif ner_dict[keys[i]] == \"O\":\n",
        "      value = 0.5 + 0.25*((1+i)/len(keys))\n",
        "    elif ner_dict[keys[i]] == \"L\":\n",
        "      value = 0.75 + 0.25*((1+i)/len(keys))\n",
        "    masks_dict[keys[i]] = value\n",
        "  return masks_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJepKhDalJ0c"
      },
      "source": [
        "def get_mask_value(mask_dict, word):\n",
        "  for key in mask_dict.keys():\n",
        "    if word in key:\n",
        "      return mask_dict[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PC18qDa-ia6"
      },
      "source": [
        "def get_entity_vector(mask_dict, token_list, current_index):\n",
        "  for key in mask_dict.keys():\n",
        "    entity_split = key.split()\n",
        "    num_words = len(entity_split)\n",
        "    if token_list[current_index] == entity_split[0]:\n",
        "      if token_list[current_index: current_index + num_words] == entity_split:\n",
        "        print(\"entity_split: \", entity_split)\n",
        "        entity_vec = [np.append(np.zeros(300), mask_dict[key])] * num_words\n",
        "        return entity_vec, num_words\n",
        "\n",
        "  return [np.zeros(301)], 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt6o334S0UOH"
      },
      "source": [
        "def createVectorListFromToken(token_list, ner_list, masks_dict_list):\n",
        "\n",
        "  max_length = 0\n",
        "  vector_list = []\n",
        "  for i in range(0,len(token_list)):\n",
        "    sentence_vector = []\n",
        "    current_token_list = token_list[i]\n",
        "    current_ner_list = ner_list[i]\n",
        "    mask_dict = masks_dict_list[i]\n",
        "\n",
        "    assert len(current_token_list) == len(current_ner_list)\n",
        "\n",
        "    # for j in range(0, len(current_token_list)):\n",
        "    j = 0\n",
        "    while j < len(current_token_list):\n",
        "      step = 1\n",
        "    \n",
        "      if (current_ner_list[j] == \"None\"):\n",
        "        try:\n",
        "          vec = [np.append(model[current_token_list[j]], 0)]\n",
        "          # print(\"current_ner_list[j] == 'None': \", vec)         \n",
        "        except KeyError:\n",
        "          vec = [np.zeros(301)]\n",
        "          # print(\"current_ner_list[j] == 'None': KeyError Occured!\")\n",
        "        \n",
        "      else:\n",
        "        try:\n",
        "          # vec = [np.append(np.zeros(300), mask_dict[current_token_list[j]])]\n",
        "          # vec = np.append(np.zeros(300), get_mask_value(mask_dict, current_token_list[j]))\n",
        "          print(\"Running `get_entity_vector()`.... for word: \", current_token_list[j])\n",
        "          vec, step = get_entity_vector(mask_dict, current_token_list, j)\n",
        "\n",
        "          # print(\"current_ner_list[j] != 'None': \", vec)\n",
        "        except KeyError:    \n",
        "          vec = [np.zeros(301)]\n",
        "          print(f\"current_ner_list[{j}] != 'None': KeyError Occured!\")\n",
        "        \n",
        "      sentence_vector.extend(vec)\n",
        "      j += step\n",
        "\n",
        "    vector_list.append(sentence_vector)\n",
        "\n",
        "  # model = None\n",
        "  return vector_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNF7GzOoyvsF"
      },
      "source": [
        "def makeTokenNERListsFromParagraph(text):\n",
        "  result = json.loads(nlp.annotate(text, properties=props))\n",
        "# def makeTokenNERListsFromParagraph(text, result):\n",
        "  sentences = result['sentences']\n",
        "  corefs = result['corefs']\n",
        "\n",
        "  sentence_tokens_test = []\n",
        "  sentence_ners_test = []\n",
        "  ner_test = {}\n",
        "\n",
        "  for each in sentences:        \n",
        "    q = each.get('tokens')\n",
        "\n",
        "    for j in range(0,len(q)):\n",
        "      sentence_tokens_test.append(q[j].get('word'))\n",
        "\n",
        "      if (q[j].get(\"ner\") == \"PERSON\" or q[j].get(\"ner\") == \"ORGANIZATION\" or q[j].get(\"ner\") == \"LOCATION\"):\n",
        "        word = build_complete_ner(q[j].get(\"word\"), q[j].get(\"ner\"), q[j+1 : len(q)])\n",
        "        if (q[j].get(\"ner\") == \"PERSON\"):\n",
        "          prefix = \"P\"\n",
        "        elif (q[j].get(\"ner\") == \"ORGANIZATION\") :\n",
        "          prefix = \"O\"\n",
        "        else:\n",
        "          prefix = \"L\"\n",
        "        if (j==0):\n",
        "          ner_test[word] = prefix\n",
        "          words = word.split(\" \")\n",
        "          for l in words:\n",
        "            sentence_ners_test.append(prefix)\n",
        "            \n",
        "        if (j!=0 and q[j-1].get(\"ner\") != q[j].get(\"ner\")):\n",
        "          ner_test[word] = prefix\n",
        "          words = word.split(\" \")\n",
        "          for l in words:\n",
        "            sentence_ners_test.append(prefix)\n",
        "          \n",
        "      else:\n",
        "        sentence_ners_test.append(\"None\")\n",
        "\n",
        "  print(\"ner_test: \", ner_test)\n",
        "  print(\"sentence_ners_test: \", sentence_ners_test)\n",
        "\n",
        "  headwords_list = []\n",
        "  for sentence in sentences:\n",
        "    headwords = []\n",
        "    for each in sentence['tokens']:\n",
        "      if(each.get(\"ner\") == \"PERSON\" or each.get(\"ner\") == \"ORGANIZATION\" or each.get(\"ner\") == \"LOCATION\"):\n",
        "        headwords.append(each.get('word'))\n",
        "      else:\n",
        "        headwords.append(0)\n",
        "\n",
        "    headwords_list.append(headwords)\n",
        "  \n",
        "  print(\"headwords_list: \", headwords_list)\n",
        "\n",
        "  final_ner_dict = {}\n",
        "  cluster_list = []\n",
        "\n",
        "  for i in corefs.values():\n",
        "    party_value = False\n",
        "    for each in ner_test.keys():\n",
        "      if each in i[0]['text']:\n",
        "        party_value = True\n",
        "        final_ner_dict[i[0]['text']] = ner_test[each]\n",
        "    if party_value == True:\n",
        "      cluster = []\n",
        "      for j in i:\n",
        "        # print(j)\n",
        "        for index in range(j['startIndex']-1,j['endIndex']-1):\n",
        "          # print(j['position'][0], index)\n",
        "          headwords_list[j['position'][0]-1][index] = i[0]['text']\n",
        "        cluster.append(j['text'])\n",
        "\n",
        "      cluster_list.append(cluster)\n",
        "\n",
        "  print(\"final_ner_dict: \", final_ner_dict)\n",
        "  print(\"cluster_list: \", cluster_list)\n",
        "  print(\"headwords_list: \", headwords_list)\n",
        "\n",
        "  final_headwords = []\n",
        "  for i in headwords_list:\n",
        "    for j in i:\n",
        "      final_headwords.append(j)\n",
        "\n",
        "  for ner in ner_test.keys():\n",
        "    val = True\n",
        "    for cluster_ner in final_ner_dict.keys():\n",
        "      if ner in cluster_ner:\n",
        "        val = False\n",
        "    if (val):\n",
        "      final_ner_dict[ner] = ner_test[ner]\n",
        "      \n",
        "  return sentence_ners_test, final_headwords, final_ner_dict, sentence_tokens_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ghx8vWgV9a"
      },
      "source": [
        "### Party Identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V0cQEeUgDHP"
      },
      "source": [
        "def Legal_entity_identifier(text):\n",
        "\n",
        "  new_text = remove_non_ascii_1(text)\n",
        "\n",
        "  ner_list, headwordsList, ner_dict, token_list = makeTokenNERListsFromParagraph(new_text)\n",
        "  mask_dict = getMaskValues(ner_dict)\n",
        "\n",
        "  vector_list = createVectorListFromToken([token_list],[ner_list],[mask_dict])\n",
        "\n",
        "  d = {x: 0 for x in ner_dict}\n",
        "\n",
        "  return new_text, d, headwordsList, vector_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjlhQJlCdWcY",
        "outputId": "f11a26e6-695b-4a10-8e40-342c3d393cb7"
      },
      "source": [
        "# f = open(\"/content/drive/Shared drives/SigmaLaw/data_2/54.txt\", \"rb\")\n",
        "# text = f.read()\n",
        "# print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Donaghy began his career as an NBA referee in September 1994 and continued in that position for thirteen seasons.  Battista agreed to pay Donaghy a fee for each game in which Donaghy correctly picked the winner. \\xe2\\x80\\x82 Donaghy provided the picks to Martino, Martino relayed the information to Battista, and Battista placed the bets. \\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjiT5basXAMP"
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MRuUHtQjmwu"
      },
      "source": [
        "def Probability_Cal(legal_entities,headwords_list,plaintif_prob,defendant_prob):\n",
        "  plaintifs=legal_entities.copy()\n",
        "  defendants=legal_entities.copy()\n",
        "  count=legal_entities.copy()\n",
        "  plaintifs_list=[]\n",
        "  defendants_list=[]\n",
        "  for i in range(0,len(headwords_list)):\n",
        "    try: \n",
        "      plaintifs[headwords_list[i]]+=plaintif_prob[i]\n",
        "      defendants[headwords_list[i]]+=defendant_prob[i]\n",
        "      count[headwords_list[i]]+=1 \n",
        "    except KeyError:  \n",
        "        continue\n",
        "    else:\n",
        "        \n",
        "       continue\n",
        "  if (len(legal_entities)>0):\n",
        "    le_list=list(legal_entities)\n",
        "    for j in range(len(legal_entities)):\n",
        "        try:\n",
        "            p=plaintifs[le_list[j]]/count[le_list[j]]\n",
        "            d=defendants[le_list[j]]/count[le_list[j]]\n",
        "        except ZeroDivisionError:\n",
        "            p=0\n",
        "            d=0\n",
        "        print(le_list[j],\" :\",p,\" , \",d)\n",
        "        if(p>=0.5 and d<0.5):\n",
        "            plaintifs_list.append(le_list[j])\n",
        "        elif(d>=0.5 and p<0.5):\n",
        "            defendants_list.append(le_list[j])\n",
        "        elif(d<0.5 and p<0.5):\n",
        "            continue\n",
        "        elif(p>d):\n",
        "            plaintifs_list.append(le_list[j])\n",
        "        elif(d>p):\n",
        "            defendants_list.append(le_list[j])\n",
        "\n",
        "    return (plaintifs_list,defendants_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OzUwerhnccN"
      },
      "source": [
        "# model1= load_model(Data_path/\"GRU_512\")\n",
        "model1 = load_model(GRU_512_MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRsvaWyrrOsT",
        "outputId": "f2da4b0b-fad3-4839-91ba-0f59719ce83f"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 443, 1024)         2503680   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 443, 2)            2050      \n",
            "=================================================================\n",
            "Total params: 2,505,730\n",
            "Trainable params: 2,505,730\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAMZeQEzKOO2"
      },
      "source": [
        "sample_txt = \"\"\"The Sacketts, petitioners here, received a compliance order from the EPA, which stated that their residential lot contained navigable waters and that their construction project violated the Act. The Sacketts sought declarative and injunctive relief in the Federal District Court, contending that the compliance order was \"arbitrary [and] capricious\" under the Administrative Procedure Act (APA), 5 U.Â S.Â C. Â§706(2)(A), and that it deprived them of due process in violation of the Fifth Amendment. The District Court dismissed the claims for want of subject-matter jurisdiction. The Ninth Circuit affirmed, concluding that the Clean Water Act precluded pre-enforcement judicial review of compliance orders and that such preclusion did not violate due process.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA6u8VYA1bBR"
      },
      "source": [
        "sample_txt = \"\"\"Dolores H. Cao, an elderly resident of Cupey, Puerto Rico, was removed from her home, made to undergo a psychological evaluation, and placed in a substitute home and, later, a state institution for the elderly, by the Puerto Rico Family Department (â€œthe Departmentâ€). â€‚ She seeks recovery under 42 U.S.C. Â§Â§â€‚1981 and 1983 for alleged violations of her procedural due process and equal protection rights, as well as under several state law causes of action. â€‚ The district court dismissed Cao's complaint under Federal Rule of Civil Procedure 12(b)(6). â€‚ After careful consideration, we affirm the district court's dismissal.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppsyTnKLzUZk"
      },
      "source": [
        "def findLegalParties(text):\n",
        "    # text=str(input(\"Enter the Legal Opinion Here: \"))    \n",
        "    print(\"\\n\")\n",
        "    new_text, legal_entities, headwords_list, vector_list = Legal_entity_identifier(text)\n",
        "    max_length = 443\n",
        "    new_list = []\n",
        "    for i in vector_list: #padding the vectors\n",
        "        for j in range(len(i),max_length):\n",
        "\n",
        "            i.append(np.zeros(301))\n",
        "        new_list.append(i)\n",
        "    vector_array = np.array(new_list)\n",
        "    k=max_length-len(headwords_list) #padding the tokens\n",
        "    for n in range(k):\n",
        "        headwords_list.append('0')\n",
        "    dec=model1.predict(vector_array)\n",
        "    dec_p=dec[:,:,0]\n",
        "    dec_d=dec[:,:,1]\n",
        "    probabilities_p = dec_p.reshape(max_length)\n",
        "    probabilities_d = dec_d.reshape(max_length)\n",
        "    bin_list_p = probabilities_p.tolist()\n",
        "    bin_list_d = probabilities_d.tolist()\n",
        "    print(\"Legal Entity : Petitioner Probability , Defendant Probability\")\n",
        "    p,d=Probability_Cal(legal_entities,headwords_list,bin_list_p,bin_list_d)\n",
        "    print(\"\\n\")\n",
        "    print(\"Petitioners:\")\n",
        "    for i in range(len(p)):\n",
        "        print(p[i])\n",
        "    print(\"\\n\") \n",
        "    print(\"Defendants:\")\n",
        "    for i in range(len(d)):\n",
        "        print(d[i]) \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mJ2O_quz42_",
        "outputId": "dca0a318-167a-4898-e329-d5b9b1cd76bc"
      },
      "source": [
        "findLegalParties(sample_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Legal Entity : Petitioner Probability , Defendant Probability\n",
            "the EPA , which stated that their residential lot contained navigable waters and that their construction project violated the Act  : 0.001403566384754782  ,  0.030603163517129734\n",
            "the Federal District Court  : 4.039289858090638e-07  ,  1.1053137392337542e-08\n",
            "APA  : 0.02320083975791931  ,  1.1728442217417978e-07\n",
            "Ninth Circuit  : 0  ,  0\n",
            "\n",
            "\n",
            "Petitioners:\n",
            "\n",
            "\n",
            "Defendants:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3KtvRlaKci2",
        "outputId": "75b481d5-1ec8-4562-ed0a-3bedb6af8ae8"
      },
      "source": [
        "findLegalParties(sample_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Legal Entity : Petitioner Probability , Defendant Probability\n",
            "the District Court 's  : 1.4041999379606352e-06  ,  4.35226128536475e-05\n",
            "Morrison  : 1.0157582329913497e-05  ,  5.441344265977939e-07\n",
            "Rose , ante , p. 5  : 2.6102001046194945e-08  ,  2.1577163171272927e-11\n",
            "The Court of Appeals for the Tenth Circuit  : 0.0005207501862393037  ,  4.071878057083151e-05\n",
            "United States Supreme Court UNITED STATES  : 0  ,  0\n",
            "Border Patrol  : 0  ,  0\n",
            "Wilson  : 1.7973914623325982e-07  ,  0.0016997456550598145\n",
            "\n",
            "\n",
            "Petitioners:\n",
            "\n",
            "\n",
            "Defendants:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU-ZamcK2Ji1",
        "outputId": "e1e4cfc6-ccff-4639-8b6b-e49201f3e56e"
      },
      "source": [
        "findLegalParties(sample_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ner_test:  {'Dolores H. Cao': 'P', 'Cupey': 'L', 'Puerto': 'L', 'Puerto Rico Family Department': 'O', 'Cao': 'P'}\n",
            "sentence_ners_test:  ['P', 'P', 'P', 'None', 'None', 'None', 'None', 'None', 'L', 'None', 'L', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'O', 'O', 'O', 'O', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'P', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
            "headwords_list:  [['Dolores', 'H.', 'Cao', 0, 0, 0, 0, 0, 'Cupey', 0, 'Puerto', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Puerto', 'Rico', 'Family', 'Department', 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 'Cao', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "final_ner_dict:  {'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico': 'P'}\n",
            "cluster_list:  [['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'her', 'She', 'her', \"Cao 's\"]]\n",
            "headwords_list:  [['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Puerto', 'Rico', 'Family', 'Department', 0, 0, 0, 0, 0], ['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "current_ner_list[0] != 'None': KeyError Occured!\n",
            "current_ner_list[1] != 'None': KeyError Occured!\n",
            "current_ner_list[2] != 'None': KeyError Occured!\n",
            "current_ner_list[8] != 'None': KeyError Occured!\n",
            "current_ner_list[10] != 'None': KeyError Occured!\n",
            "current_ner_list[45] != 'None': KeyError Occured!\n",
            "current_ner_list[46] != 'None': KeyError Occured!\n",
            "current_ner_list[47] != 'None': KeyError Occured!\n",
            "current_ner_list[48] != 'None': KeyError Occured!\n",
            "current_ner_list[91] != 'None': KeyError Occured!\n",
            "Legal Entity : Petitioner Probability , Defendant Probability\n",
            "Dolores H. Cao , an elderly resident of Cupey , Puerto Rico  : 0.3979529193157598  ,  0.012101131006961929\n",
            "Puerto Rico Family Department  : 0  ,  0\n",
            "\n",
            "\n",
            "Petitioners:\n",
            "\n",
            "\n",
            "Defendants:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSXbvadCOXD2",
        "outputId": "4bad7f90-bb21-4bc8-cede-3656e76fe065"
      },
      "source": [
        "findLegalParties(sample_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ner_test:  {'Dolores H. Cao': 'P', 'Cupey': 'L', 'Puerto': 'L', 'Puerto Rico Family Department': 'O', 'Cao': 'P'}\n",
            "sentence_ners_test:  ['P', 'P', 'P', 'None', 'None', 'None', 'None', 'None', 'L', 'None', 'L', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'O', 'O', 'O', 'O', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'P', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
            "headwords_list:  [['Dolores', 'H.', 'Cao', 0, 0, 0, 0, 0, 'Cupey', 0, 'Puerto', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Puerto', 'Rico', 'Family', 'Department', 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 'Cao', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "final_ner_dict:  {'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico': 'P'}\n",
            "cluster_list:  [['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'her', 'She', 'her', \"Cao 's\"]]\n",
            "headwords_list:  [['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Puerto', 'Rico', 'Family', 'Department', 0, 0, 0, 0, 0], ['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Running `get_entity_vector()`.... for word:  Dolores\n",
            "entity_split:  ['Dolores', 'H.', 'Cao', ',', 'an', 'elderly', 'resident', 'of', 'Cupey', ',', 'Puerto', 'Rico']\n",
            "Running `get_entity_vector()`.... for word:  Puerto\n",
            "entity_split:  ['Puerto', 'Rico', 'Family', 'Department']\n",
            "Running `get_entity_vector()`.... for word:  Cao\n",
            "Legal Entity : Petitioner Probability , Defendant Probability\n",
            "Dolores H. Cao , an elderly resident of Cupey , Puerto Rico  : 0.31171052700947105  ,  0.13479554682470016\n",
            "Puerto Rico Family Department  : 0  ,  0\n",
            "\n",
            "\n",
            "Petitioners:\n",
            "\n",
            "\n",
            "Defendants:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvnnQHiSALF4"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTEd2XTxgiFB"
      },
      "source": [
        "### Test Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LXDvX5pnSQh",
        "outputId": "bd4ba28a-340d-41d2-8df0-1011bb94254a"
      },
      "source": [
        "# model: pre-trained word2vec model\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "most_similar_key, similarity = result[0]\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "queen: 0.7118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jhBTvVdoD-X",
        "outputId": "ec97d8a1-22b4-4775-ea4b-6746eb3289d4"
      },
      "source": [
        "print(model.doesnt_match(\"football rugby ferrari basketball\".split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ferrari\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP2L1uMXgrB9"
      },
      "source": [
        "### Test Stanford Annotation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBXX30Hd8ZZ5"
      },
      "source": [
        "raw_txt = \"\"\"Dolores H. Cao, an elderly resident of Cupey, Puerto Rico, was removed from her home, made to undergo a psychological evaluation, and placed in a substitute home and, later, a state institution for the elderly, by the Puerto Rico Family Department (â€œthe Departmentâ€). â€‚ She seeks recovery under 42 U.S.C. Â§Â§â€‚1981 and 1983 for alleged violations of her procedural due process and equal protection rights, as well as under several state law causes of action. â€‚ The district court dismissed Cao's complaint under Federal Rule of Civil Procedure 12(b)(6). â€‚ After careful consideration, we affirm the district court's dismissal.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkbkjZpi8hGK",
        "outputId": "8197188d-b693-4616-b302-b7ec99923a22"
      },
      "source": [
        "ascii_txt = remove_non_ascii_1(raw_txt)\n",
        "print(ascii_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dolores H. Cao, an elderly resident of Cupey, Puerto Rico, was removed from her home, made to undergo a psychological evaluation, and placed in a substitute home and, later, a state institution for the elderly, by the Puerto Rico Family Department (the Department).  She seeks recovery under 42 U.S.C. 1981 and 1983 for alleged violations of her procedural due process and equal protection rights, as well as under several state law causes of action.  The district court dismissed Cao's complaint under Federal Rule of Civil Procedure 12(b)(6).  After careful consideration, we affirm the district court's dismissal.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M86eGq9F8trk"
      },
      "source": [
        "annotated_txt = json.loads(nlp.annotate(ascii_txt, properties=props))\n",
        "# print(annotated_txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6yu2ip8-eIB",
        "outputId": "6d053b8b-6cbd-4a8e-c872-b194dabce19d"
      },
      "source": [
        "annotated_txt.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['sentences', 'corefs'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OovbD6Zl-mu7",
        "outputId": "c9562308-2b46-47c6-e94b-dbc93bb94e87"
      },
      "source": [
        "annotated_txt['sentences'][0].get('tokens')[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'after': ' ',\n",
              " 'before': '',\n",
              " 'characterOffsetBegin': 0,\n",
              " 'characterOffsetEnd': 7,\n",
              " 'index': 1,\n",
              " 'lemma': 'Dolores',\n",
              " 'ner': 'PERSON',\n",
              " 'originalText': 'Dolores',\n",
              " 'pos': 'NNP',\n",
              " 'speaker': 'PER0',\n",
              " 'word': 'Dolores'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x4_-MJA-Vd9a",
        "outputId": "c0f15d04-c582-4907-ed08-d39bbb698604"
      },
      "source": [
        "ascii_txt[46: 52]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Puerto'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghwJdn6NfSyX",
        "outputId": "7baefa76-6ba9-4517-efc5-90ccb91bf89e"
      },
      "source": [
        "annotated_txt['corefs']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'24': [{'animacy': 'ANIMATE',\n",
              "   'endIndex': 13,\n",
              "   'gender': 'MALE',\n",
              "   'headIndex': 3,\n",
              "   'id': 1,\n",
              "   'isRepresentativeMention': True,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [1, 2],\n",
              "   'sentNum': 1,\n",
              "   'startIndex': 1,\n",
              "   'text': 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico',\n",
              "   'type': 'PROPER'},\n",
              "  {'animacy': 'ANIMATE',\n",
              "   'endIndex': 18,\n",
              "   'gender': 'FEMALE',\n",
              "   'headIndex': 17,\n",
              "   'id': 5,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [1, 6],\n",
              "   'sentNum': 1,\n",
              "   'startIndex': 17,\n",
              "   'text': 'her',\n",
              "   'type': 'PRONOMINAL'},\n",
              "  {'animacy': 'ANIMATE',\n",
              "   'endIndex': 2,\n",
              "   'gender': 'FEMALE',\n",
              "   'headIndex': 1,\n",
              "   'id': 15,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [2, 4],\n",
              "   'sentNum': 2,\n",
              "   'startIndex': 1,\n",
              "   'text': 'She',\n",
              "   'type': 'PRONOMINAL'},\n",
              "  {'animacy': 'ANIMATE',\n",
              "   'endIndex': 15,\n",
              "   'gender': 'FEMALE',\n",
              "   'headIndex': 14,\n",
              "   'id': 19,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [2, 8],\n",
              "   'sentNum': 2,\n",
              "   'startIndex': 14,\n",
              "   'text': 'her',\n",
              "   'type': 'PRONOMINAL'},\n",
              "  {'animacy': 'ANIMATE',\n",
              "   'endIndex': 7,\n",
              "   'gender': 'UNKNOWN',\n",
              "   'headIndex': 5,\n",
              "   'id': 24,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [3, 1],\n",
              "   'sentNum': 3,\n",
              "   'startIndex': 5,\n",
              "   'text': \"Cao 's\",\n",
              "   'type': 'PROPER'}],\n",
              " '32': [{'animacy': 'INANIMATE',\n",
              "   'endIndex': 4,\n",
              "   'gender': 'NEUTRAL',\n",
              "   'headIndex': 3,\n",
              "   'id': 31,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [3, 8],\n",
              "   'sentNum': 3,\n",
              "   'startIndex': 1,\n",
              "   'text': 'The district court',\n",
              "   'type': 'NOMINAL'},\n",
              "  {'animacy': 'INANIMATE',\n",
              "   'endIndex': 11,\n",
              "   'gender': 'NEUTRAL',\n",
              "   'headIndex': 9,\n",
              "   'id': 32,\n",
              "   'isRepresentativeMention': True,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [4, 1],\n",
              "   'sentNum': 4,\n",
              "   'startIndex': 7,\n",
              "   'text': \"the district court 's\",\n",
              "   'type': 'NOMINAL'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oehw8xzgwH9"
      },
      "source": [
        "### Test Main Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsXtIUVJhH7O",
        "outputId": "36d1595b-306b-4707-a04e-076f2e829b5b"
      },
      "source": [
        "ner_list, headwordsList, ner_dict, token_list = makeTokenNERListsFromParagraph(ascii_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ner_test:  {'Dolores H. Cao': 'P', 'Cupey': 'L', 'Puerto': 'L', 'Puerto Rico Family Department': 'O', 'Cao': 'P'}\n",
            "sentence_ners_test:  ['P', 'P', 'P', 'None', 'None', 'None', 'None', 'None', 'L', 'None', 'L', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'O', 'O', 'O', 'O', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'P', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
            "headwords_list:  [['Dolores', 'H.', 'Cao', 0, 0, 0, 0, 0, 'Cupey', 0, 'Puerto', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Puerto', 'Rico', 'Family', 'Department', 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 'Cao', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "final_ner_dict:  {'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico': 'P'}\n",
            "cluster_list:  [['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'her', 'She', 'her', \"Cao 's\"]]\n",
            "headwords_list:  [['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Puerto', 'Rico', 'Family', 'Department', 0, 0, 0, 0, 0], ['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R9iMwrtseHY",
        "outputId": "d0144ea7-d636-4f20-ca61-3146d6ca4b98"
      },
      "source": [
        "print(\"ner_list: \", ner_list)\n",
        "print(\"headwordsList: \", headwordsList)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ner_list:  ['P', 'P', 'P', 'None', 'None', 'None', 'None', 'None', 'L', 'None', 'L', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'O', 'O', 'O', 'O', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'P', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n",
            "headwordsList:  ['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Puerto', 'Rico', 'Family', 'Department', 0, 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtUNld3lvv18",
        "outputId": "6ccc40b5-c313-4a7e-a185-5690a1d30e78"
      },
      "source": [
        "print(\"token_list: \", token_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "token_list:  ['Dolores', 'H.', 'Cao', ',', 'an', 'elderly', 'resident', 'of', 'Cupey', ',', 'Puerto', 'Rico', ',', 'was', 'removed', 'from', 'her', 'home', ',', 'made', 'to', 'undergo', 'a', 'psychological', 'evaluation', ',', 'and', 'placed', 'in', 'a', 'substitute', 'home', 'and', ',', 'later', ',', 'a', 'state', 'institution', 'for', 'the', 'elderly', ',', 'by', 'the', 'Puerto', 'Rico', 'Family', 'Department', '-LRB-', 'the', 'Department', '-RRB-', '.', 'She', 'seeks', 'recovery', 'under', '42', 'U.S.C.', '1981', 'and', '1983', 'for', 'alleged', 'violations', 'of', 'her', 'procedural', 'due', 'process', 'and', 'equal', 'protection', 'rights', ',', 'as', 'well', 'as', 'under', 'several', 'state', 'law', 'causes', 'of', 'action', '.', 'The', 'district', 'court', 'dismissed', 'Cao', \"'s\", 'complaint', 'under', 'Federal', 'Rule', 'of', 'Civil', 'Procedure', '12', '-LRB-', 'b', '-RRB-', '-LRB-', '6', '-RRB-', '.', 'After', 'careful', 'consideration', ',', 'we', 'affirm', 'the', 'district', 'court', \"'s\", 'dismissal', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vVH_wIHv7X2",
        "outputId": "aa7004d1-d8c4-4221-ac75-76eb4e2059da"
      },
      "source": [
        "print(\"token_list[3]: \", token_list[3])\n",
        "print(\"headwordsList[3]: \", headwordsList[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "token_list[3]:  ,\n",
            "headwordsList[3]:  Dolores H. Cao , an elderly resident of Cupey , Puerto Rico\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmq3S20yipsq",
        "outputId": "aab03899-023b-401e-cb84-6e06b1cceea6"
      },
      "source": [
        "ner_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico': 'P',\n",
              " 'Puerto Rico Family Department': 'O'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3N27st5xy9w",
        "outputId": "5d63a783-bae3-4c4b-980f-7bae027f8919"
      },
      "source": [
        "le = {x: 0 for x in ner_dict}\n",
        "list(le)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico',\n",
              " 'Puerto Rico Family Department']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSXLWgu9uMN6"
      },
      "source": [
        "mask_dict = getMaskValues(ner_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHZL8DBKuOEb",
        "outputId": "2239cfb0-3284-4adf-db32-d20291e3133e"
      },
      "source": [
        "mask_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico': 0.25,\n",
              " 'Puerto Rico Family Department': 0.75}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPTlHav4uEKI",
        "outputId": "89cdce74-203d-48ba-8e6e-dd3314a259bf"
      },
      "source": [
        "vector_list = createVectorListFromToken([token_list],[ner_list],[mask_dict])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running `get_entity_vector()`.... for word:  Dolores\n",
            "entity_split:  ['Dolores', 'H.', 'Cao', ',', 'an', 'elderly', 'resident', 'of', 'Cupey', ',', 'Puerto', 'Rico']\n",
            "Running `get_entity_vector()`.... for word:  Puerto\n",
            "entity_split:  ['Puerto', 'Rico', 'Family', 'Department']\n",
            "Running `get_entity_vector()`.... for word:  Cao\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aBGlHXElnt6"
      },
      "source": [
        "mask_d = {'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico': 0.25,\n",
        " 'Puerto Rico Family Department': 0.75}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaVtXN89lswk",
        "outputId": "c63d5313-e12a-46e5-9895-75654ec26533"
      },
      "source": [
        "get_mask_value(mask_d, \"Family\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZLzYKfLg7ir"
      },
      "source": [
        "### Test Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIuYb8et02b1"
      },
      "source": [
        "def get_token_words(annotated_txt):\n",
        "  token_count = 0\n",
        "  token_paragraph = []\n",
        "  for sentence in annotated_txt['sentences']:\n",
        "    token_words = []\n",
        "    for token in sentence.get('tokens'):\n",
        "      token_words.append(token['originalText'])\n",
        "      token_count += 1\n",
        "    token_paragraph.append(token_words)\n",
        "  print(\"token count: \", token_count)\n",
        "  return token_paragraph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ANsh2A2gNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a056cee5-0f22-46c0-b9ae-e4be6ef0daa7"
      },
      "source": [
        "token_word_list = get_token_words(annotated_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "token count:  120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08YYnZgVV_sy",
        "outputId": "48d42104-9187-4e9b-fb11-5f155a62ce75"
      },
      "source": [
        "print(token_word_list[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'district', 'court', 'dismissed', 'Cao', \"'s\", 'complaint', 'under', 'Federal', 'Rule', 'of', 'Civil', 'Procedure', '12', '(', 'b', ')', '(', '6', ')', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxjBzmOuXGHz"
      },
      "source": [
        "coref_sentences = token_word_list\n",
        "for coref_list in annotated_txt['corefs'].values():\n",
        "  subject = coref_list[0]['text'].split(\",\")[0].strip()\n",
        "  for item in coref_list[1:]:\n",
        "    del coref_sentences[item['sentNum'] - 1][item['startIndex']-1 : item['endIndex']-1]\n",
        "    coref_sentences[item['sentNum'] - 1].insert(item['startIndex']-1, subject)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiK3ZCqvhG9J"
      },
      "source": [
        "coref_paragraph = \" \".join(word for sentence in coref_sentences for word in sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "LUQcv8v9l3N1",
        "outputId": "bd15b579-7e70-404d-d223-615f95b61860"
      },
      "source": [
        "coref_paragraph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico , was removed from Dolores H. Cao home , made to undergo a psychological evaluation , and placed in a substitute home and , later , a state institution for the elderly , by the Puerto Rico Family Department ( the Department ) . Dolores H. Cao seeks recovery under 42 U.S.C. 1981 and 1983 for alleged violations of Dolores H. Cao procedural due process and equal protection rights , as well as under several state law causes of action . The district court dismissed Dolores H. Cao complaint under Federal Rule of Civil Procedure 12 ( b ) ( 6 ) . After careful consideration , we affirm The district court dismissal .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NchEhpuQDHHY"
      },
      "source": [
        "def get_single_ref_locations(annotation, single_ref_petitioners, single_ref_defendants, locations_dict):\n",
        "  for sent_ind in range(len(annotation['sentences'])):\n",
        "    sentence_str = \"\"\n",
        "    token_ind = 0\n",
        "    for token in annotation['sentences'][sent_ind].get('tokens'):\n",
        "      sentence_str += token['originalText'] + token['after']\n",
        "      # print(sentence_str)\n",
        "      \n",
        "      for i in range(len(single_ref_petitioners)):\n",
        "        if single_ref_petitioners[i] in sentence_str:\n",
        "          if sent_ind not in locations_dict.keys():\n",
        "            locations_dict[sent_ind] = {'petitioner': {}, 'defendant': {}}\n",
        "          locations_dict[sent_ind]['petitioner'][token_ind] = single_ref_petitioners[i]\n",
        "          del single_ref_petitioners[i]\n",
        "          print(locations_dict)\n",
        "          break\n",
        "\n",
        "      for j in range(len(single_ref_defendants)):  \n",
        "        if single_ref_defendants[j] in sentence_str:\n",
        "          if sent_ind not in locations_dict.keys():\n",
        "            locations_dict[sent_ind] = {'petitioner': {}, 'defendant': {}}\n",
        "          locations_dict[sent_ind]['defendant'][token_ind] = single_ref_defendants[j]\n",
        "          del single_ref_defendants[j]\n",
        "          print(locations_dict)\n",
        "          break\n",
        "\n",
        "      token_ind += 1\n",
        "  \n",
        "  return locations_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gNnYJxP2-30"
      },
      "source": [
        "def get_petitioner_defendant_locations(annotation, petitioner_list, defendant_list):\n",
        "  petitioner_coref_list = []\n",
        "  defendant_coref_list = []\n",
        "  entities_in_sentences = {}\n",
        "  \"\"\"\n",
        "  {0: {'petitioner': {0: 'Dolores H. Cao', 7: 'her'}, 'defendant': {}},\n",
        "   1: {'petitioner': {0: 'she', 14: 'her'}, 'defendant': {9: 'Puerto Rico Family Department'}},\n",
        "  }\n",
        "  \"\"\"\n",
        "\n",
        "  for coref_list in annotation['corefs'].values():\n",
        "    entity = coref_list[0]['text']\n",
        "    print(\"entity: \", entity)\n",
        "    \n",
        "    if entity in petitioner_list:\n",
        "      party = 'petitioner'\n",
        "      petitioner_coref_list.append(entity)\n",
        "    elif entity in defendant_list:\n",
        "      party = 'defendant'\n",
        "      defendant_coref_list.append(entity)\n",
        "    else:\n",
        "      continue\n",
        "    \n",
        "    for item in coref_list:\n",
        "      sentence_ind = item['sentNum'] - 1\n",
        "      if sentence_ind not in entities_in_sentences.keys():\n",
        "        entities_in_sentences[sentence_ind] = {'petitioner': {}, 'defendant': {}}\n",
        "      entities_in_sentences[sentence_ind][party][item['startIndex'] - 1] = item['text']\n",
        "      print(entities_in_sentences)\n",
        "  \n",
        "  print(\"petitioner_coref_list: \", petitioner_coref_list)\n",
        "  print(\"defendant_coref_list: \", defendant_coref_list)\n",
        "  remaining_petitioners = [pet for pet in petitioner_list if pet not in petitioner_coref_list]\n",
        "  remaining_defendants = [defendant for defendant in defendant_list if defendant not in defendant_coref_list]\n",
        "\n",
        "  print(\"======== Running: get_single_ref_locations() ....\")\n",
        "\n",
        "  final_entity_locations = get_single_ref_locations(annotation, remaining_petitioners, remaining_defendants, entities_in_sentences)\n",
        "  return final_entity_locations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6unSC7j3d-6"
      },
      "source": [
        "petitioners = ['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico']\n",
        "defendants = ['Puerto Rico Family Department']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R7VPl4X3qW4",
        "outputId": "3d23b59f-9847-4bdb-b4b8-02c919118e06"
      },
      "source": [
        "entity_locations = get_petitioner_defendant_locations(annotated_txt, petitioners, defendants)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entity:  The district court\n",
            "entity:  Dolores H. Cao , an elderly resident of Cupey , Puerto Rico\n",
            "{0: {'petitioner': {0: 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico'}, 'defendant': {}}}\n",
            "{0: {'petitioner': {0: 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 16: 'her'}, 'defendant': {}}}\n",
            "{0: {'petitioner': {0: 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 16: 'her'}, 'defendant': {}}, 1: {'petitioner': {0: 'She'}, 'defendant': {}}}\n",
            "{0: {'petitioner': {0: 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 16: 'her'}, 'defendant': {}}, 1: {'petitioner': {0: 'She', 13: 'her'}, 'defendant': {}}}\n",
            "{0: {'petitioner': {0: 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 16: 'her'}, 'defendant': {}}, 1: {'petitioner': {0: 'She', 13: 'her'}, 'defendant': {}}, 2: {'petitioner': {4: \"Cao 's\"}, 'defendant': {}}}\n",
            "petitioner_coref_list:  ['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico']\n",
            "defendant_coref_list:  []\n",
            "======== Running: get_single_ref_locations() ....\n",
            "{0: {'petitioner': {0: 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 16: 'her'}, 'defendant': {48: 'Puerto Rico Family Department'}}, 1: {'petitioner': {0: 'She', 13: 'her'}, 'defendant': {}}, 2: {'petitioner': {4: \"Cao 's\"}, 'defendant': {}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAxbz5nmciJj",
        "outputId": "3f0e316b-c1cb-4f06-8b7e-e82a4b9eac67"
      },
      "source": [
        "entity_locations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'defendant': {48: 'Puerto Rico Family Department'},\n",
              "  'petitioner': {0: 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico',\n",
              "   16: 'her'}},\n",
              " 1: {'defendant': {}, 'petitioner': {0: 'She', 13: 'her'}},\n",
              " 2: {'defendant': {}, 'petitioner': {4: \"Cao 's\"}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlNJ35GXMDi4",
        "outputId": "84ee59fe-dcfe-49b0-d768-f82c4e6cfa24"
      },
      "source": [
        "sorted(entity_locations[1]['petitioner'].keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 13]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW_dP6m6NqHA"
      },
      "source": [
        "def add_occurance_count_for_repeated_elements(lst):\n",
        "  \"\"\"\n",
        "  input = ['John', 'him', 'he', 'him', 'his', 'he']\n",
        "  output = ['John', 'him_1', 'he_1', 'him_2', 'his', 'he_2']\n",
        "  \"\"\"\n",
        "  for i in range(len(lst)):\n",
        "    if lst.count(lst[i]) > 1:\n",
        "      word = lst[i]\n",
        "      same_word_count = 1\n",
        "      for j in range(i, len(lst)):\n",
        "        if lst[j] == word:\n",
        "          lst[j] = f\"{word}_{same_word_count}\"\n",
        "          same_word_count += 1\n",
        "  return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnzSnH8QLDQg"
      },
      "source": [
        "def get_sentence_wise_entities_list(annotation, location_dict):\n",
        "  ent_list = []\n",
        "  for sent_ind in range(len(annotation['sentences'])):\n",
        "    if sent_ind not in location_dict.keys():\n",
        "      ent_list.append([[], []])\n",
        "    else:\n",
        "      sentence_entities = [[], []]\n",
        "      for key in sorted(location_dict[sent_ind]['petitioner'].keys()):\n",
        "        sentence_entities[0].append(location_dict[sent_ind]['petitioner'][key])\n",
        "      for key in sorted(location_dict[sent_ind]['defendant'].keys()):\n",
        "        sentence_entities[1].append(location_dict[sent_ind]['defendant'][key])\n",
        "      \n",
        "      sentence_entities[0] = add_occurance_count_for_repeated_elements(sentence_entities[0])\n",
        "      sentence_entities[1] = add_occurance_count_for_repeated_elements(sentence_entities[1])\n",
        "      ent_list.append(sentence_entities)\n",
        "  return ent_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv13dOP9Q2-7"
      },
      "source": [
        "sentence_wise_entity_list = get_sentence_wise_entities_list(annotated_txt, entity_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRaiUEsNRHnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c279a414-f178-4d47-8872-7be8504ec954"
      },
      "source": [
        "sentence_wise_entity_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'her'],\n",
              "  ['Puerto Rico Family Department']],\n",
              " [['She', 'her'], []],\n",
              " [[\"Cao 's\"], []],\n",
              " [[], []]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bXUR9VXDtQK"
      },
      "source": [
        "a = [1,2,3,4]\n",
        "b=[1,4,5]\n",
        "c = [item for item in a if item not in b]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcEBDnXhEfJG",
        "outputId": "65682978-3ae0-4e1c-e9ef-f7643d54f700"
      },
      "source": [
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRdh_6kLhM0M"
      },
      "source": [
        "### Test CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1R9TpuijohF"
      },
      "source": [
        "pet_labels = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62X0QTNVjv1O",
        "outputId": "bb06723b-4054-4163-df5a-5bf99bf59fba"
      },
      "source": [
        "len(pet_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JHF92TGjzbb"
      },
      "source": [
        "temp = ascii_txt\n",
        "temp = temp.replace(\",\", \" ,\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "GeYgYNg6kTEm",
        "outputId": "25d05b5d-dfd9-49f6-f89f-23f593ef1e36"
      },
      "source": [
        "temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Dolores H. Cao , an elderly resident of Cupey , Puerto Rico , was removed from her home , made to undergo a psychological evaluation , and placed in a substitute home and , later , a state institution for the elderly , by the Puerto Rico Family Department (the Department).  She seeks recovery under 42 U.S.C. 1981 and 1983 for alleged violations of her procedural due process and equal protection rights , as well as under several state law causes of action.  The district court dismissed Cao's complaint under Federal Rule of Civil Procedure 12(b)(6).  After careful consideration , we affirm the district court's dismissal.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fUhGvvRkNBB",
        "outputId": "28223988-e3ca-43ff-9fb3-1fe490b1b5f0"
      },
      "source": [
        "len(temp.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}