{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Party_Extractor_separate.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PUC4Lkm_CPe8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUC4Lkm_CPe8"
      },
      "source": [
        "## Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKiy8_Uj7Mjm",
        "outputId": "b39a57b1-adce-4ccd-92cc-cbb5fc5cc63e"
      },
      "source": [
        "!wget -P /content/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-06 04:27:15--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.128.208\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.128.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/content/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  45.8MB/s    in 35s     \n",
            "\n",
            "2021-05-06 04:27:51 (44.8 MB/s) - ‘/content/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCNzggJW_KYD",
        "outputId": "c57dfd73-9c7a-4276-84de-694da71f999a"
      },
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip https://nlp.stanford.edu/software/stanford-english-corenlp-2018-10-05-models.jar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-06 04:43:20--  https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2018-10-05.zip [following]\n",
            "--2021-05-06 04:43:21--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 393239982 (375M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-full-2018-10-05.zip’\n",
            "\n",
            "stanford-corenlp-fu 100%[===================>] 375.02M  4.84MB/s    in 73s     \n",
            "\n",
            "2021-05-06 04:44:33 (5.16 MB/s) - ‘stanford-corenlp-full-2018-10-05.zip’ saved [393239982/393239982]\n",
            "\n",
            "--2021-05-06 04:44:33--  https://nlp.stanford.edu/software/stanford-english-corenlp-2018-10-05-models.jar\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-english-corenlp-2018-10-05-models.jar [following]\n",
            "--2021-05-06 04:44:33--  https://downloads.cs.stanford.edu/nlp/software/stanford-english-corenlp-2018-10-05-models.jar\n",
            "Reusing existing connection to downloads.cs.stanford.edu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1038970602 (991M) [application/java-archive]\n",
            "Saving to: ‘stanford-english-corenlp-2018-10-05-models.jar’\n",
            "\n",
            "stanford-english-co 100%[===================>] 990.84M  5.04MB/s    in 3m 18s  \n",
            "\n",
            "2021-05-06 04:47:52 (5.00 MB/s) - ‘stanford-english-corenlp-2018-10-05-models.jar’ saved [1038970602/1038970602]\n",
            "\n",
            "FINISHED --2021-05-06 04:47:52--\n",
            "Total wall clock time: 4m 31s\n",
            "Downloaded: 2 files, 1.3G in 4m 31s (5.04 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pyld-7-AW9J",
        "outputId": "6b76e845-f616-43d1-f09c-421bf12aa6d4"
      },
      "source": [
        "!unzip stanford-corenlp-full-2018-10-05.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  stanford-corenlp-full-2018-10-05.zip\n",
            "   creating: stanford-corenlp-full-2018-10-05/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/README.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2018-10-05/sutime/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.holidays.sutime.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/build.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-javadoc.jar  \n",
            "   creating: stanford-corenlp-full-2018-10-05/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2018-10-05/patterns/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/Makefile  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LICENSE.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxUG8skoAjvM"
      },
      "source": [
        "!mv stanford-english-corenlp-2018-10-05-models.jar stanford-corenlp-full-2018-10-05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnWqKpt_HGNj",
        "outputId": "a8ac3113-3db0-43ca-faf0-9ff87c0df535"
      },
      "source": [
        "!wget https://osf.io/yemvd/download"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-06 05:18:18--  https://osf.io/yemvd/download\n",
            "Resolving osf.io (osf.io)... 35.190.84.173\n",
            "Connecting to osf.io (osf.io)|35.190.84.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://files.osf.io/v1/resources/mvw7a/providers/osfstorage/6091468d19183d03395523c4?action=download&direct&version=1 [following]\n",
            "--2021-05-06 05:18:18--  https://files.osf.io/v1/resources/mvw7a/providers/osfstorage/6091468d19183d03395523c4?action=download&direct&version=1\n",
            "Resolving files.osf.io (files.osf.io)... 35.186.214.196\n",
            "Connecting to files.osf.io (files.osf.io)|35.186.214.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28223110 (27M) [application/octet-stream]\n",
            "Saving to: ‘download’\n",
            "\n",
            "download            100%[===================>]  26.92M  33.7MB/s    in 0.8s    \n",
            "\n",
            "2021-05-06 05:18:20 (33.7 MB/s) - ‘download’ saved [28223110/28223110]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApeRQj8sHQuJ",
        "outputId": "4c151fa3-e77d-4eb5-8c25-7c3e21f771fc"
      },
      "source": [
        "!unzip download"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  download\n",
            "   creating: GRU_512/assets/\n",
            "  inflating: GRU_512/variables/variables.index  \n",
            "  inflating: GRU_512/saved_model.pb  \n",
            "  inflating: GRU_512/variables/variables.data-00000-of-00001  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HU3lFWXCI4j"
      },
      "source": [
        "## Original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ4nfXBxXwd7",
        "outputId": "c2e91ebd-899e-46fe-d2a0-09623e0cb5b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7EZmmJTSOkj"
      },
      "source": [
        "from random import randint\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, Dense, Maximum, LayerNormalization\n",
        "from keras.utils import *\n",
        "from keras.initializers import *\n",
        "import tensorflow as tf\n",
        "import time, random\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_aPnjR5MgJv"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# data_path = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"Data_W2V\"\n",
        "# Data_path = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"classifier\" / \"data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN7u861oGGJB"
      },
      "source": [
        "GOOGLE_NEWS_VECTORS_PATH = \"/content/drive/MyDrive/data/GoogleNews-vectors-negative300.bin.gz\"\n",
        "STANFORD_CORENLP = \"/content/drive/MyDrive/data/stanford-corenlp-full-2018-10-05/\"\n",
        "GRU_512_MODEL = \"/content/drive/MyDrive/data/GRU_512/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPtze42TYFOA",
        "outputId": "2d772b3f-f493-4ab2-edf1-1d764dd614ec"
      },
      "source": [
        "!pip install stanfordcorenlp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanfordcorenlp\n",
            "  Downloading https://files.pythonhosted.org/packages/35/cb/0a271890bbe3a77fc1aca2bc3a58b14e11799ea77cb5f7d6fb0a8b4c46fa/stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stanfordcorenlp) (5.4.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordcorenlp) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordcorenlp) (3.0.4)\n",
            "Installing collected packages: stanfordcorenlp\n",
            "Successfully installed stanfordcorenlp-3.9.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCsq3NxOcqv-"
      },
      "source": [
        "import json\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "\n",
        "# nlp = StanfordCoreNLP(r'/content/drive/Shared drives/SigmaLaw/stanford-corenlp-full-2018-10-05', quiet=False)\n",
        "nlp = StanfordCoreNLP(STANFORD_CORENLP, quiet=False)\n",
        "props = {'annotators': 'tokenize, ner, coref', 'pipelineLanguage': 'en'}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5YNaubPxvUD"
      },
      "source": [
        "# DATA_PATH = Path.cwd().parent / \"content\" / \"drive\" / \"Shared drives\" / \"SigmaLaw\" / \"GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "import gensim\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format(DATA_PATH / 'GoogleNews-vectors-negative300.bin', binary=True, unicode_errors='ignore')\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(GOOGLE_NEWS_VECTORS_PATH, binary=True, unicode_errors='ignore')\n",
        "from gensim.models import Word2Vec "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j110IaUdFUA"
      },
      "source": [
        "def build_complete_ner(word, ner, l):\n",
        "  if (len(l)>0):\n",
        "    if (l[0].get('ner') == ner):\n",
        "      word = word + \" \" + build_complete_ner(l[0].get(\"word\"), l[0].get('ner'), l[1:len(l)])\n",
        "    #print(\"built\" + word)\n",
        "    return word\n",
        "  else:\n",
        "    return \"\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeFizBmPdPyC"
      },
      "source": [
        "def remove_non_ascii_1(text):\n",
        "  return ''.join(i for i in text if ord(i)<128)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpjzPdgC0RHn"
      },
      "source": [
        "def getMaskValues(ner_dict):\n",
        "  masks_dict = {}\n",
        "  keys = list(ner_dict.keys())\n",
        "  for i in range(len(keys)):\n",
        "    if ner_dict[keys[i]] == \"P\":\n",
        "      value = 0.5*((1+i)/len(keys))\n",
        "    elif ner_dict[keys[i]] == \"O\":\n",
        "      value = 0.5 + 0.25*((1+i)/len(keys))\n",
        "    elif ner_dict[keys[i]] == \"L\":\n",
        "      value = 0.75 + 0.25*((1+i)/len(keys))\n",
        "    masks_dict[keys[i]] = value\n",
        "  return masks_dict"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt6o334S0UOH"
      },
      "source": [
        "def createVectorListFromToken(token_list, ner_list, masks_dict_list):\n",
        "\n",
        "  max_length = 0\n",
        "  vector_list = []\n",
        "  for i in range(0,len(token_list)):\n",
        "    sentence_vector = []\n",
        "    current_token_list = token_list[i]\n",
        "    current_ner_list = ner_list[i]\n",
        "    mask_dict = masks_dict_list[i]\n",
        "\n",
        "    assert len(current_token_list) == len(current_ner_list)\n",
        "\n",
        "    for j in range(0, len(current_token_list)):\n",
        "    \n",
        "      if (current_ner_list[j] == \"None\"):\n",
        "        try:\n",
        "          vec = np.append(model[current_token_list[j]], 0)\n",
        "         \n",
        "        except KeyError:\n",
        "          vec = np.zeros(301)\n",
        "        \n",
        "      else:\n",
        "        try:\n",
        "          vec = np.append(np.zeros(300), mask_dict[current_token_list[j]])\n",
        "        except KeyError:    \n",
        "          vec = np.zeros(301)        \n",
        "        \n",
        "      sentence_vector.append(vec)\n",
        "\n",
        "    vector_list.append(sentence_vector)\n",
        "\n",
        "\n",
        "\n",
        "  return vector_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNF7GzOoyvsF"
      },
      "source": [
        "def makeTokenNERListsFromParagraph(text):\n",
        "  result = json.loads(nlp.annotate(text, properties=props))\n",
        "# def makeTokenNERListsFromParagraph(text, result):\n",
        "  sentences = result['sentences']\n",
        "  corefs = result['corefs']\n",
        "\n",
        "  sentence_tokens_test = []\n",
        "  sentence_ners_test = []\n",
        "  ner_test = {}\n",
        "  for each in sentences:\n",
        "        \n",
        "        q = each.get('tokens')\n",
        "\n",
        "        for j in range(0,len(q)):\n",
        "          sentence_tokens_test.append(q[j].get('word'))\n",
        "\n",
        "          if (q[j].get(\"ner\") == \"PERSON\" or q[j].get(\"ner\") == \"ORGANIZATION\" or q[j].get(\"ner\") == \"LOCATION\"):\n",
        "            word = build_complete_ner(q[j].get(\"word\"), q[j].get(\"ner\"), q[j+1 : len(q)])\n",
        "            if (q[j].get(\"ner\") == \"PERSON\"):\n",
        "              prefix = \"P\"\n",
        "            elif (q[j].get(\"ner\") == \"ORGANIZATION\") :\n",
        "              prefix = \"O\"\n",
        "            else:\n",
        "              prefix = \"L\"\n",
        "            if (j==0):\n",
        "              ner_test[word] = prefix\n",
        "              words = word.split(\" \")\n",
        "              for l in words:\n",
        "                sentence_ners_test.append(prefix)\n",
        "                \n",
        "            if (j!=0 and q[j-1].get(\"ner\") != q[j].get(\"ner\")):\n",
        "              ner_test[word] = prefix\n",
        "              words = word.split(\" \")\n",
        "              for l in words:\n",
        "                sentence_ners_test.append(prefix)\n",
        "              \n",
        "          else:\n",
        "            sentence_ners_test.append(\"None\")\n",
        "\n",
        "  headwords_list = []\n",
        "  for sentence in sentences:\n",
        "    headwords = []\n",
        "    for each in sentence['tokens']:\n",
        "      if(each.get(\"ner\") == \"PERSON\" or each.get(\"ner\") == \"ORGANIZATION\" or each.get(\"ner\") == \"LOCATION\"):\n",
        "        headwords.append(each.get('word'))\n",
        "      else:\n",
        "        headwords.append(0)\n",
        "\n",
        "    headwords_list.append(headwords)\n",
        "\n",
        "  final_ner_dict = {}\n",
        "\n",
        "\n",
        "  cluster_list = []\n",
        "  for i in corefs.values():\n",
        "    party_value = False\n",
        "    for each in ner_test.keys():\n",
        "      if each in i[0]['text']:\n",
        "        party_value = True\n",
        "        final_ner_dict[i[0]['text']] = ner_test[each]\n",
        "    if party_value == True:\n",
        "      cluster = []\n",
        "      for j in i:\n",
        "        # print(j)\n",
        "        for index in range(j['startIndex']-1,j['endIndex']-1):\n",
        "          # print(j['position'][0], index)\n",
        "          headwords_list[j['position'][0]-1][index] = i[0]['text']\n",
        "        cluster.append(j['text'])\n",
        "\n",
        "      cluster_list.append(cluster)\n",
        "\n",
        "  final_headwords = []\n",
        "  for i in headwords_list:\n",
        "    for j in i:\n",
        "      final_headwords.append(j)\n",
        "\n",
        "  for ner in ner_test.keys():\n",
        "    val = True\n",
        "    for cluster_ner in final_ner_dict.keys():\n",
        "      if ner in cluster_ner:\n",
        "        val = False\n",
        "    if (val):\n",
        "      final_ner_dict[ner] = ner_test[ner]\n",
        "      \n",
        "  return sentence_ners_test, final_headwords, final_ner_dict, sentence_tokens_test"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V0cQEeUgDHP"
      },
      "source": [
        "def Legal_entity_identifier(text):\n",
        "\n",
        "  new_text = remove_non_ascii_1(text)\n",
        "\n",
        "  ner_list, headwordsList, ner_dict, token_list = makeTokenNERListsFromParagraph(new_text)\n",
        "  mask_dict = getMaskValues(ner_dict)\n",
        "\n",
        "  vector_list = createVectorListFromToken([token_list],[ner_list],[mask_dict])\n",
        "\n",
        "  d = {x: 0 for x in ner_dict}\n",
        "\n",
        "  return new_text, d, headwordsList, vector_list "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjlhQJlCdWcY",
        "outputId": "f11a26e6-695b-4a10-8e40-342c3d393cb7"
      },
      "source": [
        "# f = open(\"/content/drive/Shared drives/SigmaLaw/data_2/54.txt\", \"rb\")\n",
        "# text = f.read()\n",
        "# print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Donaghy began his career as an NBA referee in September 1994 and continued in that position for thirteen seasons.  Battista agreed to pay Donaghy a fee for each game in which Donaghy correctly picked the winner. \\xe2\\x80\\x82 Donaghy provided the picks to Martino, Martino relayed the information to Battista, and Battista placed the bets. \\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjiT5basXAMP"
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MRuUHtQjmwu"
      },
      "source": [
        "def Probability_Cal(legal_entities,headwords_list,plaintif_prob,defendant_prob):\n",
        "  plaintifs=legal_entities.copy()\n",
        "  defendants=legal_entities.copy()\n",
        "  count=legal_entities.copy()\n",
        "  plaintifs_list=[]\n",
        "  defendants_list=[]\n",
        "  for i in range(0,len(headwords_list)):\n",
        "    try: \n",
        "      plaintifs[headwords_list[i]]+=plaintif_prob[i]\n",
        "      defendants[headwords_list[i]]+=defendant_prob[i]\n",
        "      count[headwords_list[i]]+=1 \n",
        "    except KeyError:  \n",
        "        continue\n",
        "    else:\n",
        "        \n",
        "       continue\n",
        "  if (len(legal_entities)>0):\n",
        "    le_list=list(legal_entities)\n",
        "    for j in range(len(legal_entities)):\n",
        "        try:\n",
        "            p=plaintifs[le_list[j]]/count[le_list[j]]\n",
        "            d=defendants[le_list[j]]/count[le_list[j]]\n",
        "        except ZeroDivisionError:\n",
        "            p=0\n",
        "            d=0\n",
        "        print(le_list[j],\" :\",p,\" , \",d)\n",
        "        if(p>=0.5 and d<0.5):\n",
        "            plaintifs_list.append(le_list[j])\n",
        "        elif(d>=0.5 and p<0.5):\n",
        "            defendants_list.append(le_list[j])\n",
        "        elif(d<0.5 and p<0.5):\n",
        "            continue\n",
        "        elif(p>d):\n",
        "            plaintifs_list.append(le_list[j])\n",
        "        elif(d>p):\n",
        "            defendants_list.append(le_list[j])\n",
        "\n",
        "    return (plaintifs_list,defendants_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OzUwerhnccN"
      },
      "source": [
        "# model1= load_model(Data_path/\"GRU_512\")\n",
        "model1 = load_model(GRU_512_MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAMZeQEzKOO2"
      },
      "source": [
        "sample_txt = \"\"\"The Sacketts, petitioners here, received a compliance order from the EPA, which stated that their residential lot contained navigable waters and that their construction project violated the Act. The Sacketts sought declarative and injunctive relief in the Federal District Court, contending that the compliance order was \"arbitrary [and] capricious\" under the Administrative Procedure Act (APA), 5 U.Â S.Â C. Â§706(2)(A), and that it deprived them of due process in violation of the Fifth Amendment. The District Court dismissed the claims for want of subject-matter jurisdiction. The Ninth Circuit affirmed, concluding that the Clean Water Act precluded pre-enforcement judicial review of compliance orders and that such preclusion did not violate due process.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppsyTnKLzUZk"
      },
      "source": [
        "def findLegalParties():\n",
        "    # text=str(input(\"Enter the Legal Opinion Here: \"))\n",
        "    text = sample_txt\n",
        "    print(\"\\n\")\n",
        "    new_text, legal_entities, headwords_list, vector_list = Legal_entity_identifier(text)\n",
        "    max_length = 443\n",
        "    new_list = []\n",
        "    for i in vector_list: #padding the vectors\n",
        "        for j in range(len(i),max_length):\n",
        "\n",
        "            i.append(np.zeros(301))\n",
        "        new_list.append(i)\n",
        "    vector_array = np.array(new_list)\n",
        "    k=max_length-len(headwords_list) #padding the tokens\n",
        "    for n in range(k):\n",
        "        headwords_list.append('0')\n",
        "    dec=model1.predict(vector_array)\n",
        "    dec_p=dec[:,:,0]\n",
        "    dec_d=dec[:,:,1]\n",
        "    probabilities_p = dec_p.reshape(max_length)\n",
        "    probabilities_d = dec_d.reshape(max_length)\n",
        "    bin_list_p = probabilities_p.tolist()\n",
        "    bin_list_d = probabilities_d.tolist()\n",
        "    print(\"Legal Entity : Petitioner Probability , Defendant Probability\")\n",
        "    p,d=Probability_Cal(legal_entities,headwords_list,bin_list_p,bin_list_d)\n",
        "    print(\"\\n\")\n",
        "    print(\"Petitioners:\")\n",
        "    for i in range(len(p)):\n",
        "        print(p[i])\n",
        "    print(\"\\n\") \n",
        "    print(\"Defendants:\")\n",
        "    for i in range(len(d)):\n",
        "        print(d[i]) \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mJ2O_quz42_",
        "outputId": "dca0a318-167a-4898-e329-d5b9b1cd76bc"
      },
      "source": [
        "findLegalParties()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Legal Entity : Petitioner Probability , Defendant Probability\n",
            "the EPA , which stated that their residential lot contained navigable waters and that their construction project violated the Act  : 0.001403566384754782  ,  0.030603163517129734\n",
            "the Federal District Court  : 4.039289858090638e-07  ,  1.1053137392337542e-08\n",
            "APA  : 0.02320083975791931  ,  1.1728442217417978e-07\n",
            "Ninth Circuit  : 0  ,  0\n",
            "\n",
            "\n",
            "Petitioners:\n",
            "\n",
            "\n",
            "Defendants:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3KtvRlaKci2",
        "outputId": "75b481d5-1ec8-4562-ed0a-3bedb6af8ae8"
      },
      "source": [
        "findLegalParties()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Legal Entity : Petitioner Probability , Defendant Probability\n",
            "the District Court 's  : 1.4041999379606352e-06  ,  4.35226128536475e-05\n",
            "Morrison  : 1.0157582329913497e-05  ,  5.441344265977939e-07\n",
            "Rose , ante , p. 5  : 2.6102001046194945e-08  ,  2.1577163171272927e-11\n",
            "The Court of Appeals for the Tenth Circuit  : 0.0005207501862393037  ,  4.071878057083151e-05\n",
            "United States Supreme Court UNITED STATES  : 0  ,  0\n",
            "Border Patrol  : 0  ,  0\n",
            "Wilson  : 1.7973914623325982e-07  ,  0.0016997456550598145\n",
            "\n",
            "\n",
            "Petitioners:\n",
            "\n",
            "\n",
            "Defendants:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvnnQHiSALF4"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBXX30Hd8ZZ5"
      },
      "source": [
        "raw_txt = \"\"\"Dolores H. Cao, an elderly resident of Cupey, Puerto Rico, was removed from her home, made to undergo a psychological evaluation, and placed in a substitute home and, later, a state institution for the elderly, by the Puerto Rico Family Department (â€œthe Departmentâ€). â€‚ She seeks recovery under 42 U.S.C. Â§Â§â€‚1981 and 1983 for alleged violations of her procedural due process and equal protection rights, as well as under several state law causes of action. â€‚ The district court dismissed Cao's complaint under Federal Rule of Civil Procedure 12(b)(6). â€‚ After careful consideration, we affirm the district court's dismissal.\"\"\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkbkjZpi8hGK",
        "outputId": "189849ce-8e0a-454d-b525-a6385f48bad9"
      },
      "source": [
        "ascii_txt = remove_non_ascii_1(raw_txt)\n",
        "print(ascii_txt)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dolores H. Cao, an elderly resident of Cupey, Puerto Rico, was removed from her home, made to undergo a psychological evaluation, and placed in a substitute home and, later, a state institution for the elderly, by the Puerto Rico Family Department (the Department).  She seeks recovery under 42 U.S.C. 1981 and 1983 for alleged violations of her procedural due process and equal protection rights, as well as under several state law causes of action.  The district court dismissed Cao's complaint under Federal Rule of Civil Procedure 12(b)(6).  After careful consideration, we affirm the district court's dismissal.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M86eGq9F8trk"
      },
      "source": [
        "annotated_txt = json.loads(nlp.annotate(ascii_txt, properties=props))\n",
        "# print(annotated_txt)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6yu2ip8-eIB",
        "outputId": "c06ce6a2-a953-466d-8b40-d4a6951f9a34"
      },
      "source": [
        "annotated_txt.keys()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['sentences', 'corefs'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OovbD6Zl-mu7",
        "outputId": "c707eb13-18b6-47f2-bb28-1fe21b8d35ba"
      },
      "source": [
        "annotated_txt['sentences'][0].get('tokens')[3]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'after': ' ',\n",
              " 'before': '',\n",
              " 'characterOffsetBegin': 14,\n",
              " 'characterOffsetEnd': 15,\n",
              " 'index': 4,\n",
              " 'lemma': ',',\n",
              " 'ner': 'O',\n",
              " 'originalText': ',',\n",
              " 'pos': ',',\n",
              " 'speaker': 'PER0',\n",
              " 'word': ','}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghwJdn6NfSyX",
        "outputId": "f4d076ac-6895-47ac-e5b8-9522191a9797"
      },
      "source": [
        "annotated_txt['corefs']"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'24': [{'animacy': 'ANIMATE',\n",
              "   'endIndex': 13,\n",
              "   'gender': 'MALE',\n",
              "   'headIndex': 3,\n",
              "   'id': 1,\n",
              "   'isRepresentativeMention': True,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [1, 2],\n",
              "   'sentNum': 1,\n",
              "   'startIndex': 1,\n",
              "   'text': 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico',\n",
              "   'type': 'PROPER'},\n",
              "  {'animacy': 'ANIMATE',\n",
              "   'endIndex': 18,\n",
              "   'gender': 'FEMALE',\n",
              "   'headIndex': 17,\n",
              "   'id': 5,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [1, 6],\n",
              "   'sentNum': 1,\n",
              "   'startIndex': 17,\n",
              "   'text': 'her',\n",
              "   'type': 'PRONOMINAL'},\n",
              "  {'animacy': 'ANIMATE',\n",
              "   'endIndex': 2,\n",
              "   'gender': 'FEMALE',\n",
              "   'headIndex': 1,\n",
              "   'id': 15,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [2, 4],\n",
              "   'sentNum': 2,\n",
              "   'startIndex': 1,\n",
              "   'text': 'She',\n",
              "   'type': 'PRONOMINAL'},\n",
              "  {'animacy': 'ANIMATE',\n",
              "   'endIndex': 15,\n",
              "   'gender': 'FEMALE',\n",
              "   'headIndex': 14,\n",
              "   'id': 19,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [2, 8],\n",
              "   'sentNum': 2,\n",
              "   'startIndex': 14,\n",
              "   'text': 'her',\n",
              "   'type': 'PRONOMINAL'},\n",
              "  {'animacy': 'ANIMATE',\n",
              "   'endIndex': 7,\n",
              "   'gender': 'UNKNOWN',\n",
              "   'headIndex': 5,\n",
              "   'id': 24,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [3, 1],\n",
              "   'sentNum': 3,\n",
              "   'startIndex': 5,\n",
              "   'text': \"Cao 's\",\n",
              "   'type': 'PROPER'}],\n",
              " '32': [{'animacy': 'INANIMATE',\n",
              "   'endIndex': 4,\n",
              "   'gender': 'NEUTRAL',\n",
              "   'headIndex': 3,\n",
              "   'id': 31,\n",
              "   'isRepresentativeMention': False,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [3, 8],\n",
              "   'sentNum': 3,\n",
              "   'startIndex': 1,\n",
              "   'text': 'The district court',\n",
              "   'type': 'NOMINAL'},\n",
              "  {'animacy': 'INANIMATE',\n",
              "   'endIndex': 11,\n",
              "   'gender': 'NEUTRAL',\n",
              "   'headIndex': 9,\n",
              "   'id': 32,\n",
              "   'isRepresentativeMention': True,\n",
              "   'number': 'SINGULAR',\n",
              "   'position': [4, 1],\n",
              "   'sentNum': 4,\n",
              "   'startIndex': 7,\n",
              "   'text': \"the district court 's\",\n",
              "   'type': 'NOMINAL'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsXtIUVJhH7O",
        "outputId": "be72f7f2-a5a3-4846-b87e-2f9d0d033ded"
      },
      "source": [
        "ner_list, headwordsList, ner_dict, token_list = makeTokenNERListsFromParagraph(ascii_txt)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': 1, 'text': 'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico', 'type': 'PROPER', 'number': 'SINGULAR', 'gender': 'MALE', 'animacy': 'ANIMATE', 'startIndex': 1, 'endIndex': 13, 'headIndex': 3, 'sentNum': 1, 'position': [1, 2], 'isRepresentativeMention': True}\n",
            "{'id': 5, 'text': 'her', 'type': 'PRONOMINAL', 'number': 'SINGULAR', 'gender': 'FEMALE', 'animacy': 'ANIMATE', 'startIndex': 17, 'endIndex': 18, 'headIndex': 17, 'sentNum': 1, 'position': [1, 6], 'isRepresentativeMention': False}\n",
            "{'id': 15, 'text': 'She', 'type': 'PRONOMINAL', 'number': 'SINGULAR', 'gender': 'FEMALE', 'animacy': 'ANIMATE', 'startIndex': 1, 'endIndex': 2, 'headIndex': 1, 'sentNum': 2, 'position': [2, 4], 'isRepresentativeMention': False}\n",
            "{'id': 19, 'text': 'her', 'type': 'PRONOMINAL', 'number': 'SINGULAR', 'gender': 'FEMALE', 'animacy': 'ANIMATE', 'startIndex': 14, 'endIndex': 15, 'headIndex': 14, 'sentNum': 2, 'position': [2, 8], 'isRepresentativeMention': False}\n",
            "{'id': 24, 'text': \"Cao 's\", 'type': 'PROPER', 'number': 'SINGULAR', 'gender': 'UNKNOWN', 'animacy': 'ANIMATE', 'startIndex': 5, 'endIndex': 7, 'headIndex': 5, 'sentNum': 3, 'position': [3, 1], 'isRepresentativeMention': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmq3S20yipsq",
        "outputId": "74f42c44-0968-4680-b9d2-0084b13ab4d3"
      },
      "source": [
        "ner_dict"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dolores H. Cao , an elderly resident of Cupey , Puerto Rico': 'P',\n",
              " 'Puerto Rico Family Department': 'O'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}